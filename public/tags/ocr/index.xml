<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ocr on Nathan Lane, PhD</title>
    <link>http://example.com/tags/ocr/</link>
    <description>Recent content in Ocr on Nathan Lane, PhD</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <copyright>All rights reserved - 2018</copyright>
    <lastBuildDate>Sat, 06 Dec 2014 13:42:46 +0000</lastBuildDate>
    
	<atom:link href="http://example.com/tags/ocr/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tutorial: Training an OCR Engine</title>
      <link>http://example.com/post/2014-12-06-training-an-ocr-engine-to-recognize-old-stuff-abbyy-finereader/</link>
      <pubDate>Sat, 06 Dec 2014 13:42:46 +0000</pubDate>
      
      <guid>http://example.com/post/2014-12-06-training-an-ocr-engine-to-recognize-old-stuff-abbyy-finereader/</guid>
      <description>In a previous tutorial I covered the basics of digitizing old stats with ABBYY FineReader (&amp;amp; alternative digitization tools). Now, I dig into some important digitization nitty gritty: training optical character recognition software to properly read historical content.
&amp;nbsp;
Most historical digitization projects will entail training. Old statistical documents often use long-gone proprietary typefaces. While modern OCR software can easily read Arials and Times New Romans, it needs help with more exotic typography; this is where training come in.</description>
    </item>
    
    <item>
      <title>Tutorial: Manipulating PDFs in Python (to Scrape Them).</title>
      <link>http://example.com/post/2014-10-19-chopping-and-merging-pdfs-in-python-to-scrape-them/</link>
      <pubDate>Sun, 19 Oct 2014 16:16:08 +0000</pubDate>
      
      <guid>http://example.com/post/2014-10-19-chopping-and-merging-pdfs-in-python-to-scrape-them/</guid>
      <description>When digitizing old data, we often start with a pile of scanned documents we must reorganize. Much time is spent manually trudging through scans, deducing what variables exist, and selecting the tables we eventually wish to turn into machine-readable data. When you have hundreds of multi-page PDFS, this can be a painful experience. However, automating PDF manipulation with Python can save major time.
The Problem We start with scans of old, provincial statistical yearbooks for a Southeast Asian country.</description>
    </item>
    
    <item>
      <title>Tutorial: A Beginner&#39;s Guide to Scraping Historic Table Data</title>
      <link>http://example.com/post/2014-10-05-a-basic-tutorial-for-digitizing-historic-tabular-data/</link>
      <pubDate>Sun, 05 Oct 2014 02:05:28 +0000</pubDate>
      
      <guid>http://example.com/post/2014-10-05-a-basic-tutorial-for-digitizing-historic-tabular-data/</guid>
      <description>This is a simple introduction to scraping tables from historic (scanned) documents. It is by no means definitive. Instead, this is a broad overview aimed at researchers with minimal programming experience tackling smaller digitization projects--say, nothing more than 200 pages. I focus on OCRing material with ABBYY FineReader, a popular commercial program for OCRing. ABBYY has a relatively gentle learning curve and, importantly, straightforward table functionality. &amp;nbsp;  For those more comfortable with the command line and programming, or for open source advocates, I suggest free programmatic alternatives for each tutorial step.</description>
    </item>
    
  </channel>
</rss>