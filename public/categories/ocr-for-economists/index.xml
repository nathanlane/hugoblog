<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ocr for Economists on Nathan Lane, PhD</title>
    <link>http://example.com/categories/ocr-for-economists/</link>
    <description>Recent content in Ocr for Economists on Nathan Lane, PhD</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <copyright>All rights reserved - 2018</copyright>
    <lastBuildDate>Sat, 06 Dec 2014 13:42:46 +0000</lastBuildDate>
    
	<atom:link href="http://example.com/categories/ocr-for-economists/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tutorial: Training an OCR Engine</title>
      <link>http://example.com/post/2014-12-06-training-an-ocr-engine-to-recognize-old-stuff-abbyy-finereader/</link>
      <pubDate>Sat, 06 Dec 2014 13:42:46 +0000</pubDate>
      
      <guid>http://example.com/post/2014-12-06-training-an-ocr-engine-to-recognize-old-stuff-abbyy-finereader/</guid>
      <description>In a previous tutorial I covered the basics of digitizing old stats with ABBYY FineReader (&amp;amp; alternative digitization tools). Now, I dig into some important digitization nitty gritty: training optical character recognition software to properly read historical content.
&amp;nbsp;
Most historical digitization projects will entail training. Old statistical documents often use long-gone proprietary typefaces. While modern OCR software can easily read Arials and Times New Romans, it needs help with more exotic typography; this is where training come in.</description>
    </item>
    
    <item>
      <title>Tutorial: A Beginner&#39;s Guide to Scraping Historic Table Data</title>
      <link>http://example.com/post/2014-10-05-a-basic-tutorial-for-digitizing-historic-tabular-data/</link>
      <pubDate>Sun, 05 Oct 2014 02:05:28 +0000</pubDate>
      
      <guid>http://example.com/post/2014-10-05-a-basic-tutorial-for-digitizing-historic-tabular-data/</guid>
      <description>This is a simple introduction to scraping tables from historic (scanned) documents. It is by no means definitive. Instead, this is a broad overview aimed at researchers with minimal programming experience tackling smaller digitization projects--say, nothing more than 200 pages. I focus on OCRing material with ABBYY FineReader, a popular commercial program for OCRing. ABBYY has a relatively gentle learning curve and, importantly, straightforward table functionality. &amp;nbsp;  For those more comfortable with the command line and programming, or for open source advocates, I suggest free programmatic alternatives for each tutorial step.</description>
    </item>
    
    <item>
      <title>A great primer on cleaning OCRd data with Python &amp; Regular Expressions.</title>
      <link>http://example.com/post/2014-09-03-a-great-primer-of-cleaning-ocrd-data-with-python-regular-expressions/</link>
      <pubDate>Wed, 03 Sep 2014 06:04:44 +0000</pubDate>
      
      <guid>http://example.com/post/2014-09-03-a-great-primer-of-cleaning-ocrd-data-with-python-regular-expressions/</guid>
      <description>Link: Cleaning OCR’d text with Regular Expressions Often the pain of optical character recognition isn&#39;t the OCRing procedure itself, it is cleaning the tiny, little inconsistencies that plague OCRd content. This is especially true when we OCR historical material: even high quality scans can have a speckle or two that get recognized as gibberish.
Adept use of Regular Expressions (regex) coupled with simple Python (or Ruby scripts--or heck, even Notepad++) can be a powerful means of removing nasty errors from OCRd text/CSV files.</description>
    </item>
    
  </channel>
</rss>