--- author: Nathaniel categories: - Political Economy date:
"2014-09-30T03:29:47Z" meta: \_edit\_last: "1" pe\_theme\_meta:
O:8:"stdClass":2:{s:7:"gallery";O:8:"stdClass":3:{s:2:"id";s:2:"89";s:5:"width";s:0:"";s:6:"height";s:0:"";}s:5:"video";O:8:"stdClass":1:{s:2:"id";s:3:"262";}}
published: true status: publish tags: - crowdsourcing - data - methods -
political science title: Text Analysis, (Non-) Experts, and Turning
"Stuff" into Social Science Data type: post ---

Social scientists often use experts to "code" datasets; experts read
some stuff and code that stuff using a coding rule (a right-left
political spectrum, etc.). But this a slow, painful process for
constructing a dataset

Below is an awesome, succinct Kickstarter seminar on how to categorize
(political) text using crowd-sourcing from quant-y political scientist,
Drew Conway (<http://drewconway.com/>):

The great lil' seminar is related to his recent working paper, "Methods
for Collecting Large-scale Non-expert Text Coding":

##### Abstract

> The task of coding text for discrete categories or quantifiable scales
> is a classic problem in political science. Traditionally, this task is
> executed by qualified \`\`experts''. While productive, this method is
> time consuming, resource intensive, and introduces bias. In the
> following paper I present the findings from a series of experiments
> developed to assess the viability of using crowd-sourcing platforms
> for political text coding, and how variations in the collection
> mechanism affects the quality of output...
